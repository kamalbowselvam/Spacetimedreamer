<!DOCTYPE html>
<html lang="en">
    <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
        
        

        <script async src="https://www.googletagmanager.com/gtag/js?id=G-T976NQLENM"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-T976NQLENM');
        </script>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
        <title>Gradient Descent - Kamal SELVAM</title><meta name="Description" content="Implementation of Gradient Descent Algorithm"><meta property="og:url" content="http://localhost:1313/posts/02_gradient-descent/">
  <meta property="og:site_name" content="Kamal SELVAM">
  <meta property="og:title" content="Gradient Descent">
  <meta property="og:description" content="Implementation of Gradient Descent Algorithm">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
  <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2019-12-11T21:57:40+08:00">
    <meta property="article:modified_time" content="2020-01-01T16:45:40+08:00">
    <meta property="article:tag" content="Machine Learning">
    <meta property="article:tag" content="Python">
    <meta property="og:image" content="http://localhost:1313/posts/02_gradient-descent/fit.gif">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://localhost:1313/posts/02_gradient-descent/fit.gif"><meta name="twitter:title" content="Gradient Descent">
<meta name="twitter:description" content="Implementation of Gradient Descent Algorithm">
      <meta name="twitter:site" content="@kamalbowselvam">
<meta name="application-name" content="Kamal SELVAM">
<meta name="apple-mobile-web-app-title" content="Kamal SELVAM"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="http://localhost:1313/posts/02_gradient-descent/" /><link rel="prev" href="http://localhost:1313/posts/01_lorentz-attractor/" /><link rel="next" href="http://localhost:1313/posts/03_perceptron/" /><link rel="stylesheet" href="/lib/normalize/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Gradient Descent",
        "inLanguage": "en",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "http:\/\/localhost:1313\/posts\/02_gradient-descent\/"
        },"image": [{
                            "@type": "ImageObject",
                            "url": "http:\/\/localhost:1313\/posts\/02_gradient-descent\/fit.gif",
                            "width":  1000 ,
                            "height":  300 
                        }],"genre": "posts","keywords": "Machine Learning, Python","wordcount":  1111 ,
        "url": "http:\/\/localhost:1313\/posts\/02_gradient-descent\/","datePublished": "2019-12-11T21:57:40+08:00","dateModified": "2020-01-01T16:45:40+08:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": "xxxx","logo": {
                    "@type": "ImageObject",
                    "url": "http:\/\/localhost:1313\/images\/avatar.png",
                    "width":  528 ,
                    "height":  560 
                }},"author": {
                "@type": "Person",
                "name": "Kamal SELVAM"
            },"description": "Implementation of Gradient Descent Algorithm"
    }
    </script></head>
    <body header-desktop="fixed" header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('dark' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'dark' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Kamal SELVAM"><span id="id-1" class="typeit"></span></a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> Posts </a><a class="menu-item" href="/tags/"> Tags </a><a class="menu-item" href="/categories/"> Categories </a><a class="menu-item" href="/about/"> About </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="Clear">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Kamal SELVAM"><span id="id-2" class="typeit"></span></a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="Search">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="Clear">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        Cancel
                    </a>
                </div><a class="menu-item" href="/posts/" title="">Posts</a><a class="menu-item" href="/tags/" title="">Tags</a><a class="menu-item" href="/categories/" title="">Categories</a><a class="menu-item" href="/about/" title="">About</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw"></i>
            </a></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>
<main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animated flipInX">Gradient Descent</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://kamalselvam.com" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw"></i>Kamal SELVAM</a></span>&nbsp;<span class="post-category">included in <a href="/categories/machine-learning/"><i class="far fa-folder fa-fw"></i>Machine Learning</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2019-12-11">2019-12-11</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;1111 words&nbsp;
                <i class="far fa-clock fa-fw"></i>&nbsp;6 minutes&nbsp;</div>
        </div><div class="featured-image"><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/posts/02_gradient-descent/fit.gif"
        data-srcset="/posts/02_gradient-descent/fit.gif, /posts/02_gradient-descent/fit.gif 1.5x, /posts/02_gradient-descent/fit.gif 2x"
        data-sizes="auto"
        alt="/posts/02_gradient-descent/fit.gif"
        title="Implementation of Gradient Descent Algorithm" /></div><div class="details toc" id="toc-static"  kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#the-gradient">The Gradient</a></li>
    <li><a href="#gradient-descent-algorithm">Gradient Descent Algorithm</a></li>
    <li><a href="#implementation">Implementation</a></li>
    <li><a href="#linear-regression-cost-function">Linear Regression Cost Function</a></li>
    <li><a href="#conclusion">Conclusion</a></li>
    <li><a href="#reference">Reference</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><h2 id="introduction">Introduction</h2>
<p>Gradient descent is a fundamental optimization algorithm widely used in machine learning and optimization problems. It is employed to minimize a function by iteratively moving in the direction of the steepest descent as indicated by the negative of the gradient. This article aims to explain the concepts behind gradient descent and its implementation in python.</p>
<p>Before diving into gradient descent, it&rsquo;s crucial to understand the notion of optimization. In optimization, the goal is to find the minimum or maximum of a function. For simplicity, let&rsquo;s focus on minimizing a function, typically denoted as $ f(x) $, where $ x $ represents the parameters of the function. The process of finding the minimum of $  f(x)  $ involves iterative steps towards adjusting the parameters $  x  $ until reaching a minimum.</p>
<h2 id="the-gradient">The Gradient</h2>
<p>The gradient of a function, denoted as $ \nabla f(x) $, is a vector that points in the direction of the steepest increase of the function at a particular point. In other words, it indicates the direction in which the function grows fastest. The negative gradient, $ -\nabla f(x) $, points in the direction of the steepest decrease, which is the direction of the greatest decrease of the function.</p>
<h2 id="gradient-descent-algorithm">Gradient Descent Algorithm</h2>
<p>Gradient descent operates by iteratively updating the parameters $ x $ in the opposite direction of the gradient of the function $ f(x) $ with respect to $ x $. The update rule for gradient descent can be represented as:</p>
<p>$$  x_{t+1} = x_t - \alpha \nabla f(x_t)  $$</p>
<p>where:</p>
<ul>
<li>$  x_t  $ represents the parameters at iteration $  t  $.</li>
<li>$  \alpha $ (alpha) denotes the learning rate, which controls the step size or the rate at which the parameters are updated.</li>
</ul>
<p>The learning rate is a critical hyperparameter in gradient descent. A too small learning rate may result in slow convergence, while a too large learning rate can cause divergence, where the optimization process fails to converge to a minimum.</p>
<h2 id="implementation">Implementation</h2>
<p>Initially a synthetic data is generated via the function randomDataGenerator. Using np.arange(), an array of x values ranging from 0 to 5 with a step size of 0.01 is created. Then, the slope and bias value are sampled from a normal distribution of mean 1 and 5 respectively. Later, y values for each x value using the equation of a straight line: $ y = \theta _{0} * x + \theta _{1} $.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">randomDataGenerator</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param theta0: slope
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param theta1: bias
</span></span></span><span class="line"><span class="cl"><span class="s2">    :return: data x,y
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">n_rnd</span> <span class="o">=</span> <span class="mi">500</span>
</span></span><span class="line"><span class="cl">    <span class="n">theta0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_rnd</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">theta1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_rnd</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">y</span> <span class="o">=</span> <span class="n">theta0_init</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">theta1_init</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">x</span> <span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta0_init</span><span class="p">,</span> <span class="n">theta1_init</span>
</span></span></code></pre></div><p> 
<figure><a class="lightgallery" href="/posts/02_gradient-descent/synthetic.png" title="synthetic" data-thumbnail="/posts/02_gradient-descent/synthetic.png" data-sub-html="<h2> Figure 1: Synthetic data generated using $ y = \theta _{0} * x &#43; \theta _{1} $ </h2><p>synthetic</p>">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="synthetic.png"
            data-srcset="/posts/02_gradient-descent/synthetic.png, synthetic.png 1.5x, /posts/02_gradient-descent/synthetic.png 2x"
            data-sizes="auto"
            alt="/posts/02_gradient-descent/synthetic.png" />
    </a><figcaption class="image-caption"> Figure 1: Synthetic data generated using $ y = \theta _{0} * x + \theta _{1} $ </figcaption>
    </figure></p>
<h2 id="linear-regression-cost-function">Linear Regression Cost Function</h2>
<p>Once the data is generated, A cost function for linear regression is defined to apply gradient descent, often termed the Mean Squared Error (MSE), quantifies the model&rsquo;s performance by calculating the average of the squared differences between predicted and actual values across all training examples.</p>
<p>The formula for the MSE cost function is as follows:</p>
<blockquote>
<p>$$ \tag{1} J(\theta) = \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x_{i}) - y_{i})^{2} $$</p>
</blockquote>
<p>Here,</p>
<ul>
<li>$ J(\theta) $ is the cost function.</li>
<li>$ m $ is the number of training examples.</li>
<li>$ h_{\theta}(x_{i}) $ represents the predicted value of the i-th example using parameters $  \theta $.</li>
<li>$ y_{i} $ is the actual value of the i-th example.</li>
</ul>
<p>The parameters are the coefficients (slope and intercept) of the linear equation. To perform gradient descent, we need the partial derivatives of the cost function with respect to each parameter. These derivatives indicate the direction and magnitude of the parameter update that will decrease the cost <a href="#1" rel="">[1]</a>.</p>
<p>Partial derivative with respect to the intercept $ \theta_{0} $:</p>
<blockquote>
<p>$$ \tag{2} \frac{\partial}{\partial \theta _{0}} J (\theta) = \frac{2}{m} \sum _{i=1}^{m} (h _{\theta}(x _{i}) - y _{i})$$</p>
</blockquote>
<p>Partial derivative with respect to the slope $ \theta_{1} $:</p>
<blockquote>
<p>$$ \tag{3} \frac{\partial }{\partial \theta _{1}}  J (\theta) = \frac {2}{m} \sum _{i=1}^{m} ( h _{\theta}(x _{i}) - y _{i} )  \cdot x _{i} $$</p>
</blockquote>
<p>These equations show how much the cost function will change if we change each parameter slightly. The hypothesis used to create the synthetic data can be denoted as $ H( \theta)  $. To obtain the model parameters, we employ gradient descent by taking the derivative of the function with respect to the parameters. The following function implements the gradient descent algorithm. At each iteration, the parameters are updated by subtracting the derivative multiplied by the learning rate $ \alpha $. Convergence of the algorithm is determined by a threshold, which measures the change in parameter values between two consecutive iterations.&quot;</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">hypothesis</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta0</span><span class="p">,</span> <span class="n">theta1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">theta0</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">theta1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">gradientDescent</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">c</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">all_m</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="n">all_c</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="n">threshold</span> <span class="o">=</span> <span class="mi">999999</span>
</span></span><span class="line"><span class="cl">    <span class="k">while</span> <span class="n">threshold</span> <span class="o">&gt;</span> <span class="mf">10e-20</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">d_m</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">hypothesis</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">m</span><span class="p">,</span><span class="n">c</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span> <span class="p">)</span> <span class="o">*</span> <span class="n">x</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">d_c</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">hypothesis</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">m</span><span class="p">,</span><span class="n">c</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span> <span class="p">))</span> <span class="o">*</span>  <span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">old_m</span> <span class="o">=</span> <span class="n">m</span>
</span></span><span class="line"><span class="cl">        <span class="n">all_m</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">all_c</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">m</span> <span class="o">=</span> <span class="n">m</span> <span class="o">-</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">d_m</span>
</span></span><span class="line"><span class="cl">        <span class="n">c</span> <span class="o">=</span> <span class="n">c</span> <span class="o">-</span> <span class="mf">0.01</span>  <span class="o">*</span> <span class="n">d_c</span>
</span></span><span class="line"><span class="cl">        <span class="n">threshold</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">old_m</span> <span class="o">-</span> <span class="n">m</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">i</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span><span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">c</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">all_m</span><span class="p">,</span> <span class="n">all_c</span>
</span></span></code></pre></div><p>The values of the slope and the bias are stored in a variable, that is later used for visualization. The code below uses all the function defined above to create and fit a linea regression model using gradient descent algroithm</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">dataX</span><span class="p">,</span> <span class="n">dataY</span><span class="p">,</span> <span class="n">theta0_init</span><span class="p">,</span> <span class="n">theta1_init</span> <span class="o">=</span> <span class="n">randomDataGenerator</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">theta0_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">101</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">theta1_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">101</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">Z</span> <span class="o">=</span> <span class="n">cost_func_3d</span><span class="p">(</span><span class="n">theta0_grid</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="n">theta1_grid</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="n">dataX</span><span class="p">,</span><span class="n">dataY</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">theta0_grid</span><span class="p">,</span> <span class="n">theta1_grid</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">all_m</span><span class="p">,</span> <span class="n">all_c</span> <span class="o">=</span> <span class="n">gradientDescent</span><span class="p">(</span><span class="n">m</span><span class="o">=-</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">dataX</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">dataY</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Contour Plot for Grandient Descent Convergence&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Theta 0&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Theta 1&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mf">0.9992458979704978</span><span class="p">,</span> <span class="mf">5.015957571468284</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">line</span><span class="p">,</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([],</span> <span class="p">[],</span><span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">annotation</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">700000</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">annotation</span><span class="o">.</span><span class="n">set_animated</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">anim</span> <span class="o">=</span> <span class="n">animation</span><span class="o">.</span><span class="n">FuncAnimation</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">animate</span><span class="p">,</span> <span class="n">init_func</span><span class="o">=</span><span class="n">init</span><span class="p">,</span> <span class="n">frames</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">blit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">writer</span> <span class="o">=</span> <span class="n">animation</span><span class="o">.</span><span class="n">PillowWriter</span><span class="p">(</span><span class="n">fps</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span><span class="n">metadata</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">artist</span><span class="o">=</span><span class="s1">&#39;Me&#39;</span><span class="p">),</span><span class="n">bitrate</span><span class="o">=</span><span class="mi">1800</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">anim</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;contour.gif&#39;</span><span class="p">,</span> <span class="n">writer</span><span class="o">=</span><span class="n">writer</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p>The converge of the algorithm for the synthetic data can be seen in below Figure</p>
<p><figure><a class="lightgallery" href="/posts/02_gradient-descent/fit.gif" title="Lorentrz" data-thumbnail="/posts/02_gradient-descent/fit.gif" data-sub-html="<h2> Figure 3: Convergence of gradient descent algorithm on the synthetic data</h2><p>Lorentrz</p>">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="fit.gif"
            data-srcset="/posts/02_gradient-descent/fit.gif, fit.gif 1.5x, /posts/02_gradient-descent/fit.gif 2x"
            data-sizes="auto"
            alt="/posts/02_gradient-descent/fit.gif" />
    </a><figcaption class="image-caption"> Figure 3: Convergence of gradient descent algorithm on the synthetic data</figcaption>
    </figure></p>
<p>The convergence of the model parameters could be visualized using contour plot as show below</p>
<p><figure><a class="lightgallery" href="/posts/02_gradient-descent/scatter.gif" title="Lorentr" data-thumbnail="/posts/02_gradient-descent/scatter.gif" data-sub-html="<h2> Figure 3: Countor plot converegence of model parameters</h2><p>Lorentr</p>">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="scatter.gif"
            data-srcset="/posts/02_gradient-descent/scatter.gif, scatter.gif 1.5x, /posts/02_gradient-descent/scatter.gif 2x"
            data-sizes="auto"
            alt="/posts/02_gradient-descent/scatter.gif" />
    </a><figcaption class="image-caption"> Figure 3: Countor plot converegence of model parameters</figcaption>
    </figure></p>
<h2 id="conclusion">Conclusion</h2>
<p>Gradient descent is a powerful optimization algorithm used to minimize functions iteratively by moving in the direction of the steepest descent. Understanding gradient descent and its variants is essential for practitioners in machine learning and optimization fields, as it underpins many modern optimization techniques and algorithms. With proper tuning of hyperparameters and careful consideration of the problem domain, gradient descent can efficiently solve a wide range of optimization problems.</p>
<h2 id="reference">Reference</h2>
<p><!-- raw HTML omitted -->[1]<!-- raw HTML omitted --> : <a href="https://mccormickml.com/2014/03/04/gradient-descent-derivation/" target="_blank" rel="noopener noreffer">Gradient Descent Derivation</a></p>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2020-01-01</span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="Share on Twitter" data-sharer="twitter" data-url="http://localhost:1313/posts/02_gradient-descent/" data-title="Gradient Descent" data-via="kamalbowselvam" data-hashtags="Machine Learning,Python"><i class="fab fa-twitter fa-fw"></i></a><a href="javascript:void(0);" title="Share on Facebook" data-sharer="facebook" data-url="http://localhost:1313/posts/02_gradient-descent/" data-hashtag="Machine Learning"><i class="fab fa-facebook-square fa-fw"></i></a><a href="javascript:void(0);" title="Share on Linkedin" data-sharer="linkedin" data-url="http://localhost:1313/posts/02_gradient-descent/"><i class="fab fa-linkedin fa-fw"></i></a><a href="javascript:void(0);" title="Share on Hacker News" data-sharer="hackernews" data-url="http://localhost:1313/posts/02_gradient-descent/" data-title="Gradient Descent"><i class="fab fa-hacker-news fa-fw"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw"></i>&nbsp;<a href="/tags/machine-learning/">Machine Learning</a>,&nbsp;<a href="/tags/python/">Python</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/posts/01_lorentz-attractor/" class="prev" rel="prev" title="Lorentz Attractor"><i class="fas fa-angle-left fa-fw"></i>Lorentz Attractor</a>
            <a href="/posts/03_perceptron/" class="next" rel="next" title="Perceptron">Perceptron<i class="fas fa-angle-right fa-fw"></i></a></div>
</div>
</article></div>
            </main></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/lightgallery/lightgallery.min.css"><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/katex/copy-tex.min.css"><script type="text/javascript" src="/lib/smooth-scroll/smooth-scroll.min.js"></script><script type="text/javascript" src="/lib/autocomplete/autocomplete.min.js"></script><script type="text/javascript" src="/lib/algoliasearch/algoliasearch-lite.umd.min.js"></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js"></script><script type="text/javascript" src="/lib/lightgallery/lightgallery.min.js"></script><script type="text/javascript" src="/lib/lightgallery/lg-thumbnail.min.js"></script><script type="text/javascript" src="/lib/lightgallery/lg-zoom.min.js"></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="/lib/sharer/sharer.min.js"></script><script type="text/javascript" src="/lib/typeit/typeit.min.js"></script><script type="text/javascript" src="/lib/katex/katex.min.js"></script><script type="text/javascript" src="/lib/katex/auto-render.min.js"></script><script type="text/javascript" src="/lib/katex/copy-tex.min.js"></script><script type="text/javascript" src="/lib/katex/mhchem.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":10},"comment":{},"data":{"id-1":"cd ~","id-2":"cd ~"},"lightGallery":{"actualSize":false,"exThumbImage":"data-thumbnail","hideBarsDelay":2000,"selector":".lightgallery","speed":400,"thumbContHeight":80,"thumbWidth":80,"thumbnail":true},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"PASDMWALPK","algoliaIndex":"index.en","algoliaSearchKey":"b42948e51daaa93df92381c8e2ac0f93","highlightTag":"em","maxResultLength":10,"noResultsFound":"No results found","snippetLength":30,"type":"algolia"},"typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"id-1":["id-1"],"id-2":["id-2"]},"duration":-1,"speed":100}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
