<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Machine Learning - Tag - Kamal SELVAM</title>
        <link>http://localhost:1313/tags/machine-learning/</link>
        <description>Machine Learning - Tag - Kamal SELVAM</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Mon, 22 Jun 2020 21:57:40 &#43;0800</lastBuildDate><atom:link href="http://localhost:1313/tags/machine-learning/" rel="self" type="application/rss+xml" /><item>
    <title>Perceptron</title>
    <link>http://localhost:1313/posts/03_perceptron/</link>
    <pubDate>Mon, 22 Jun 2020 21:57:40 &#43;0800</pubDate>
    <author>Author</author>
    <guid>http://localhost:1313/posts/03_perceptron/</guid>
    <description><![CDATA[Artcle in development ]]></description>
</item><item>
    <title>Gradient Descent</title>
    <link>http://localhost:1313/posts/02_gradient-descent/</link>
    <pubDate>Wed, 11 Dec 2019 21:57:40 &#43;0800</pubDate>
    <author>Author</author>
    <guid>http://localhost:1313/posts/02_gradient-descent/</guid>
    <description><![CDATA[Introduction Gradient descent is a fundamental optimization algorithm widely used in machine learning and optimization problems. It is employed to minimize a function by iteratively moving in the direction of the steepest descent as indicated by the negative of the gradient. This article aims to explain the concepts behind gradient descent and its implementation in python.
Before diving into gradient descent, it&rsquo;s crucial to understand the notion of optimization. In optimization, the goal is to find the minimum or maximum of a function.]]></description>
</item></channel>
</rss>
